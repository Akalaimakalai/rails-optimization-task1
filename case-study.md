# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики

Была сформулирована в задаче:
Добиться того, чтобы программа корректно обработала файл data_large.txt за 30 секунд

Метрика: Время обработки файла data_large.txt

Бюджет: 30 секунд

## Данные по асимптотике

1000: 0.072944 x0
2000: 0.222400 x3
4000: 0.764133 x3,5
8000: 2.832144 x4
16000: 16.096870 x5,7

попробуем взять файл в 16000 записей и заставить его работать за 1 секунду.

# Шаг 1

Тестовое количество записей: 16_000
Время: 15.8
Точка роста: #select
Изменения: группируем сессии по user_id до перебора массива пользователей
Новое время:  0.8

# Шаг 2

Тестовое количество записей: 64_000
Время: 8.8
Точка роста: #fill_users_sessions#each
Изменения: обработка больших массивов через #each_slice, предварительна группировка обектов пользователей и сессий
Новое время: 2.6

# Шаг 3

Тестовое количество записей: 64_000
Время: 2.6
Точка роста: #uniq_browsers#all
Изменения: избавились от #all в бользу #uniq
Новое время: 1.6

# Шаг 4

Тестовое количество записей: 64_000
Время: 1.6
Точка роста: #collect_stats_from_users#map
Изменения: уменьшено количество вызовов #map, убран Date.parse(date).iso8601 (я не понял зачем он нужен)
Новое время: 0.7

# Шаг 4

Тестовое количество записей: 128_000
Время: 1.5
Точка роста: 
Изменения: 
Новое время: 
